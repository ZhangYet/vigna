#+TITLE: bt 调度笔记
#+OPTIONS: ^:nil
#+HTML_HEAD: <link rel="stylesheet" href="https://latex.now.sh/style.css">
* 准备工作

`sched_class` 在 kernel/sched/sched.h 里面定义。

bt 调度在 kernel/sched/batch.c 里面定义。
* 数据结构
#+BEGIN_SRC c
struct bt_rq {
	struct load_weight load;
	unsigned int nr_running, h_nr_running;
	unsigned long nr_uninterruptible;

	u64 exec_clock;
	u64 min_vruntime;
#ifndef CONFIG_64BIT
	u64 min_vruntime_copy;
#endif

	struct rb_root_cached tasks_timeline;
	struct rb_node *rb_leftmost;

	/*
	 * 'curr' points to currently running entity on this bt_rq.
	 * It is set to NULL otherwise (i.e when none are currently running).
	 */
	struct sched_entity *curr, *next, *last, *skip;

#ifdef	CONFIG_SCHED_DEBUG
	unsigned int nr_spread_over;
#endif

#ifdef CONFIG_SMP
/*
 * Load-tracking only depends on SMP, BT_GROUP_SCHED dependency below may be
 * removed when useful for applications beyond shares distribution (e.g.
 * load-balance).
 */
	/*
	 * BT Load tracking
	 */
	struct sched_avg_bt avg;
	u64 runnable_load_sum;
	unsigned long runnable_load_avg;

#ifdef CONFIG_BT_GROUP_SCHED
	unsigned long tg_load_avg_contrib;
#endif /* CONFIG_BT_GROUP_SCHED */
	atomic_long_t removed_load_avg, removed_util_avg;
#ifndef CONFIG_64BIT
	u64 load_last_update_time_copy;
#endif

	/*
	 *   h_load = weight * f(tg)
	 *
	 * Where f(tg) is the recursive weight fraction assigned to
	 * this group.
	 */
	unsigned long h_load;
#endif /* CONFIG_SMP */
#ifdef CONFIG_BT_GROUP_SCHED
	struct rq *rq;  /* cpu runqueue to which this cfs_rq is attached */

	/*
	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
	 * (like users, containers etc.)
	 *
	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
	 * list is used during load balance.
	 */
	int on_list;
	struct list_head leaf_bt_rq_list;
	struct task_group *tg;  /* group that "owns" this runqueue */
#endif /* CONFIG_BT_GROUP_SCHED */

	int bt_throttled;
	u64 bt_time;
	u64 bt_runtime;

	u64 throttled_clock, throttled_clock_task;
	u64 throttled_clock_task_time;

	/* Nests inside the rq lock: */
	raw_spinlock_t bt_runtime_lock;
};
#+END_SRC
* 入口
~bt_sched_class~ 在 batch.c 定义，它的 next 是 ~idle_sched_class~ ,

~sched_fork()~ 最后调用 ~bt_sched_class~ 的 ~task_fork_bt~ 函数。

#+BEGIN_SRC c
static void task_fork_bt(struct task_struct *p)
{
        // 创建一个 bt_rq 对象
	struct bt_rq *bt_rq;
	// se 指向 curr （我痛恨逗号）
	struct sched_entity *se = &p->bt, *curr;
	// 禁止抢占的 get_cpu() 返回当前使用的 cpu id
	// 见 https://www.kernel.org/doc/htmldocs/kernel-hacking/routines-processorids.html
	int this_cpu = smp_processor_id();
	// the macro this_rq() returns the runqueue of the current processor; 
	// and the macro task_rq(task) returns a pointer to the runqueue on which the given task is queued.
	struct rq *rq = this_rq();
	unsigned long flags;
	// 跟中断有关的 lock
	// 见 LKD P149
	raw_spin_lock_irqsave(&rq->lock, flags);
	
	update_rq_clock(rq);

	bt_rq = task_bt_rq(current);
	curr = bt_rq->curr;

	/*
	 * Not only the cpu but also the task_group of the parent might have
	 * been changed after parent->se.parent,cfs_rq were copied to
	 * child->se.parent,cfs_rq. So call __set_task_cpu() to make those
	 * of child point to valid ones.
	 */
	rcu_read_lock();
	__set_task_cpu(p, this_cpu);
	rcu_read_unlock();

	update_curr_bt(bt_rq);

	if (curr)
		se->vruntime = curr->vruntime;
	place_bt_entity(bt_rq, se, 1);

	if (sysctl_sched_child_runs_first && curr && bt_entity_before(curr, se)) {
		/*
		 * Upon rescheduling, sched_class::put_prev_task() will place
		 * 'current' within the tree based on its new key value.
		 */
		swap(curr->vruntime, se->vruntime);
		resched_curr(rq);
	}

	se->vruntime -= bt_rq->min_vruntime;

	raw_spin_unlock_irqrestore(&rq->lock, flags);
}

#+END_SRC
